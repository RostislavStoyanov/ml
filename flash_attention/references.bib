@article{dao_faster_nodate,
	title = {Faster {Attention} with {Better} {Parallelism} and {Work} {Partitioning}},
	language = {en},
	author = {Dao, Tri},
	url = {https://tridao.me/publications/flash2/flash2.pdf},
}

@misc{dao_flashattention_2022,
	title = {{FlashAttention}: {Fast} and {Memory}-{Efficient} {Exact} {Attention} with {IO}-{Awareness}},
	shorttitle = {{FlashAttention}},
	url = {http://arxiv.org/abs/2205.14135},
	doi = {10.48550/arXiv.2205.14135},
	urldate = {2025-05-24},
	publisher = {arXiv},
	author = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and Ré, Christopher},
	month = jun,
	year = {2022},
}

@misc{cuda_intro_guide,
	title = {1. {Introduction} — {CUDA} {C}++ {Programming} {Guide}},
	url = {https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#},
	urldate = {2025-05-25},
}

@book{wen2022programming,
  title={Programming massively parallel processors: a hands-on approach},
  author={Wen-Mei, W Hwu and Kirk, David B and El Hajj, Izzat},
  year={2022},
  publisher={Morgan Kaufmann}
}

@article{ye_online_nodate,
	title = {From {Online} {Softmax} to {FlashAttention}},
	language = {en},
	author = {Ye, Zihao},
	url = {https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf}
}
